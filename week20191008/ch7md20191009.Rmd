---
title: "CH 7 MD"
author: "Matthew Swanson"
date: "20191009"
output:
    bookdown::html_document2:
    highlight: textmate
    theme: yeti
---

```{r setup, include = FALSE}
library(dplyr)
```

# Introduction

We’ll use the `Credit` dataframe from the `ISLR` package to demonstrate multiple regression with:

1. A numerical outcome variable $y$, in this case credit card balance.
2. Two explanatory variables:
    * A first numerical explanatory variable $x_1$. In this case, their credit limit.
    * A second numerical explanatory variable $x_2$. In this case, their income (in thousands of dollars).

# Exploratory data analysis

* Load the `Credit` data
* Use the `View` command to look at raw data.
* Now `select()` only `Balance`, `Limit`, `Income`, `Rating` and `Age` variables. (We will be using `Rating` and `Age` in a forthcoming exercise)

```{r}
library(ISLR)
data(Credit)
#View(Credit)
glimpse(Credit)
credit = Credit %>% select(Balance, Limit, Income, Rating, Age)
glimpse(credit)
```

Let’s look at some summary statistics for the variables that we need for the problem at hand.

```{r}
library(skimr)
library(ggplot2)
library(cowplot)

credit %>% skim()

pAge = ggplot(data = credit, mapping = aes(x = Age)) +
  geom_histogram(binwidth = 10, fill = 'red2', color = 'red4')

pBalance = ggplot(data = credit, mapping = aes(x = Balance)) +
  geom_histogram(binwidth = 200, fill = 'blue2', color = 'blue4')

pLimit = ggplot(data = credit, mapping = aes(x = Limit)) +
  geom_histogram(binwidth = 1000, fill = 'yellow2', color = 'yellow4')

pRating = ggplot(data = credit, mapping = aes(x = Rating)) +
  geom_histogram(binwidth = 100, fill = 'green2', color = 'green4')

plot_grid(pAge, pBalance, pLimit, pRating)
```

Let’s also look at *histograms* as visual aids.

We observe for example:

* *The average credit card balance is $459.50.*
* *25% of card holders had debts of $68.75 or less.*
* *The average credit card limit is $4622.50.*
* *75% of these card holders had incomes of $57,470 or less.*
* 50% of card holders have debts over $459.50
* The average age of card holders is 56 years old
* 


Since our outcome variable `Balance` and the explanatory variables `Limit` and `Income` are numerical, we can and have to compute the *correlation coefficient* between pairs of these variables before we proceed to build a model.

```{r}
credit %>%
  select(Balance, Limit, Income) %>% 
  cor()
```

* `Balance` with `Limit` is $0.862$. This indicates a strong positive linear relationship, which makes sense as only individuals with large credit limits can accrue large credit card balances.
* `Balance` with `Income` is $0.464$. This is suggestive of another positive linear relationship, although not as strong as the relationship between Balance and Limit.
* As an added bonus, we can read off the correlation coefficient of the two explanatory variables, `Limit` and `Income` of $0.792$. In this case, we say there is a high degree of collinearity between these two explanatory variables.

> Note: Collinearity (or multicollinearity) is a phenomenon in which one explanatory variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy. So in this case, if we knew someone’s credit card Limit and since Limit and Income are highly correlated, we could make a fairly accurate guess as to that person’s Income. Or put loosely, these two variables provided redundant information. For now let’s ignore any issues related to collinearity and press on.

Let’s visualize the relationship of the outcome variable with each of the two explanatory variables in two separate plots:

To get a sense of the joint relationship of all three variables simultaneously through a visualization, let’s display the data in a 3-dimensional (3D) scatterplot, where

1. The numerical outcome variable $y$ `Balance` is on the $z$-axis (vertical axis)
2. The two numerical explanatory variables form the “floor” axes. In this case
    * The first numerical explanatory variable $x_1$, `Income` is on of the floor axes.
    * The second numerical explanatory variable $x_2$, `Limit` is on the other floor axis.

```{r}
library(plotly)
p <- plot_ly(data = Credit, z = ~Balance, x = ~Income, y = ~Limit, opacity = 0.6, color = Credit$Balance) %>%
  add_markers() 
p
```

### Exercise

Conduct a new exploratory data analysis with the same outcome variable $y$ being `Balance` but with `Rating` and `Age` as the new explanatory variables $x_1$ and $x_2$. Remember, this involves three things:

1. Looking at the raw values
2. Computing summary statistics of the variables of interest.
    * *Half of cardholders are over the age of 56.*
    * *25% of cardholders have a credit rating of under 247.25*
    * *Only a quarter of cardholders are under 42 years old*
    * *25% of cardholders are over 70 years old*
    * *25% of cardholders have a credit rating over 437.25*
    * *The youngest credit card holder is 23 years old*
    * *The oldest credit card holder is 98 years old*
    * *The lowest credit card rating is 93*
    * *The highest credit card rating is 982*
3. Creating informative visualizations

What can you say about the relationship between a credit card holder’s balance and their credit rating and age?

```{r}
credit %>%
  select(Balance, Rating, Age) %>% 
  cor()

p <- plot_ly(data = Credit, z = ~Balance, x = ~Rating, y = ~Age, opacity = 0.6, color = Credit$Balance) %>%
    add_markers() 
p
```

Balance with Limit is 0.862. This indicates a strong positive linear relationship, which makes sense as only individuals with large credit limits can accrue large credit card balances.
Balance with Income is 0.464. This is suggestive of another positive linear relationship, although not as strong as the relationship between Balance and Limit.
As an added bonus, we can read off the correlation coefficient of the two explanatory variables, Limit and Income of 0.792. In this case, we say there is a high degree of collinearity between these two explanatory variables.

## Multiple regression

We now use a `+` to consider multiple explanatory variables. Here is the syntax:

    model_name <- lm(y ~ x1 + x2 + ... +xn, data = data_frame_name)

```{r}
Balance_model <- lm(Balance ~ Limit + Income, data = Credit)
Balance_model

# Or use one of the followings to see more info...
library(moderndive)
get_regression_table(Balance_model)

#summary(Balance_model)
```

> Model: $\hat{Balance}= -385 +  0.264 \cdot Limit - 7.66 \cdot Income$

How do we interpret these three values that define the regression plane?

* `Intercept`: $-\$385.18$ (rounded to two decimal points to represent cents). The intercept in our case represents the credit card balance for an individual who has both a credit `Limit` of $\$0$ and `Income` of $\$0$.
    * In our data however, the intercept has limited (or no) practical interpretation as ….
* `Limit`: $\$0.26$. Now that we have multiple variables to consider, we have to add a caveat to our interpretation: Holding all the other variables fixed (`Income`, in this case), for every increase of one unit in credit `Limit` (dollars), there is an associated increase of on average $\$0.26$ in credit card balance.
* `Income`: $-\$7.66$. Similarly, Holding all the other variables fixed (`Limit`, in this case), for every increase of one unit in `Income` (in other words, $\$10000$ in income), there is an associated decrease of on average $\$7.66$ in credit card balance.

## Observed/fitted values and residuals

As we did previously, let’s look at the fitted values and residuals.

```{r}
regression_points <- get_regression_points(Balance_model)
regression_points
```

### Diagnostics (Residual plot)

```{r}
ggplot(Balance_model, aes(x = .fitted, y = .resid)) + geom_point()
```

### Making predictions

Assuming the model is good...

Kevin has a credit limit of $5080 and his income is $150,000. Use the Balance_model to predict Kevin’s balance.

    newx <- data.frame(Limit = _____, Income = ____)

    predicted_balance <- predict(Balance_model, newx)
    predicted_balance

``` {r}
newx = data.frame(Limit = c(5080,4090), Income = c(15,30))
predicted_balance = predict(Balance_model, newx)
predicted_balance
```

## One numerical & one categorical explanatory variable

Let’s revisit the instructor evaluation data introduced in Ch 6.

Consider a modeling scenario with:

1. A numerical outcome variable $y$. As before, instructor evaluation score.
2. Two explanatory variables:
    a. A numerical explanatory variable $x_1$: in this case, their `age`.
    b. A categorical explanatory variable $x_2$: in this case, their binary `gender`.
    
### Exploratory data analysis

Let’s reload the `evals` data and `select()` only the needed subset of variables. Note that these are different than the variables chosen in Chapter 6. Let’s given this the name `evals_ch7`.

1. Let’s look at the raw data values both by using View() and the glimpse() functions.
2. Let’s look at some summary statistics using the skim() function from the `skimr` package:

```{r}
library(moderndive)
data(evals)

evals_ch7 = evals %>% select(age, gender, score)

glimpse(evals_ch7)

evals_ch7 %>% skim()
```

3. Let’s compute the correlation between two numerical variables we have `score` and `age`. Recall that correlation coefficients only exist between *numerical* variables.

```{r}
evals_ch7 %>% 
  get_correlation(formula = score ~ age)
```

We observe that the `age` and the `score` are *weakly* and *negatively* correlated.

Now, let’s try to visualize the correlation.

* Create a scatterplot of `score` over `age`. Use the binary `gender` variable to color the point with two colors. Add a regression line (or two) in to your scatterplot.
    * Say a couple of interesting things about the graph you’ve created.

```{r}
ggplot(data = evals_ch7, mapping = aes(x = age, y = score, color = gender)) + 
    geom_point(alpha = 0.5) +
    labs(x = "Age", y = "Teaching Score", title = "Using method = lm") +
    geom_smooth(method = "lm", se = FALSE)
```

* Females experience a more severe decline in teaching score with age
* Many of the poor teaching scores for men come from men over 55, while many of the poor teaching scores for women come from women between 35 and 55

### Multiple regression: Parallel slopes model

Much like we started to consider multiple explanatory variables using the `+` sign in the previous section, let’s fit a regression model and get the regression table.

```{r}
score_model_ch7 = lm(score ~ age + gender, data = evals_ch7)
score_model_ch7

#get_regression_table(score_model_ch7)
```

Full: $\hat{Score} = 4.48 - 0.009 \cdot age + 0.191 \cdot 1_{Male}(x)$

Male: $\hat{Score_M} = 4.671 - 0.009 \cdot age$

Female: $\hat{Score_F} = 4.48 - 0.009 \cdot age$

Let’s create the scatterplot of `score` over `age` again. Use the binary `gender` variable to color the point with two colors. Add a regression lines in to your scatterplot but use the model(s) we created.

``` {r}
ggplot(data = evals_ch7, mapping = aes(x = age, y = score, color = gender)) + 
    geom_point(alpha = 0.5) +
    labs(x = "Age", y = "Teaching Score", title = "Using Parallel Slopes") +
    geom_abline(intercept = 4.48, slope = -0.009, color = "tomato", lwd=1) +
    geom_abline(intercept = 4.671, slope = -0.009, color = "mediumturquoise", lwd=1)
```

Interpretaions of the coefficients:

* $b_{male}=0.1906$ is the average difference in teaching score that men get relative to the baseline of women.
* Accordingly, the intercepts (which in this case make no sense since no instructor can have an age of 0) are :
    * for women: $b_0=4.484$
    * for men: $b_0+b_{male}=4.484+0.191=4.675$ 
* Both men and women have the same slope. In other words, in this model the associated effect of age is the same for men and women. So for every increase of one year in age, there is on average an associated decrease of $b_{age}=−0.009$ in teaching score.

### Multiple Regression: Interaction Model

We say a model has an interaction effect if the associated effect of one variable depends on the value of another variable. These types of models usually prove to be tricky to view on first glance because of their complexity. In this case, the effect of age will depend on the value of gender. (as was suggested by the different slopes for men and women in our visual exploratory data analysis)

Let’s fit a regression with an interaction term. We add an interaction term using the `*` sign. Let’s fit this regression and save it in `score_model_interaction`, then we get the regression table using the `get_regression_table()` function as before.

```{r}
score_model_interaction <- lm(score ~ age + gender + age * gender, data = evals_ch7)
get_regression_table(score_model_interaction)
```

The modeling equation for this scenario is (Writing the equation):

> $\hat{y} = b_0 + b_1 \cdot x_1 + b_2 \cdot x_2 + b_3 \cdot x_1 \cdot x_2$

> $\hat{score} = 4.883 - 0.018 \cdot age - 0.446 \cdot 1_{Male}(x) + 0.014   \cdot age \cdot 1_{Male}(x)$

The model for male:

> $\hat{score} = 4.883 - 0.018 \cdot age - 0.446 \cdot 1 + 0.014 \cdot age \cdot 1$

> $\hat{score} = 4.437 - 0.004 \cdot age$

The model for female:

> $\hat{score} = 4.883 - 0.018 \cdot age - 0.446 \cdot 0 + 0.014 \cdot age \cdot 0$

> $\hat{score} = 4.883 - 0.018 \cdot age$

We see that while male instructors have a lower intercept, as they age, they have a less steep associated average decrease in teaching scores: 0.004 teaching score units per year as opposed to -0.018 for women. This is consistent with the different slopes and intercepts of the red and blue regression lines fit in the original scatter plot.

``` {r}
ggplot(data = evals_ch7, mapping = aes(x = age, y = score, color = gender)) + 
    geom_point(alpha = 0.5) +
    labs(x = "Age", y = "Teaching Score", title = "Using Parallel Slopes") +
    geom_abline(intercept = 4.883, slope = -0.018, color = "tomato", lwd=1) +
    geom_abline(intercept = 4.437, slope = -0.004, color = "mediumturquoise", lwd=1)
```

### Observed/fitted values and residuals

```{r}
regression_points <- get_regression_points(score_model_interaction)
regression_points

ggplot(score_model_interaction, aes(x = .fitted, y = .resid)) + geom_point()
```

